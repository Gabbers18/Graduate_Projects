---
title: "Gambling the Game of Probabilities that may Provide Insight into Non-Probablitistic Behavior"
subtitle: "Final Project"
author: "Gabrielle Young"
date: "2024-04-25"
format:
  html:
    self-contained: true
---

# Introduction

Recently, I have noticed a spike in gambling, specifically online gambling, 
in friend groups I associate with. This could be because I am from Las Vegas, 
or maybe because the majority of my friends have now turned 21 and can legally
gamble, or maybe for some other reason I have not considered. Perhaps it is 
because gambling is addictive. From my understanding, gambling can be just as
addictive, if not more addictive than actual substances. 

I will be exploring this dataset I found on Kaggle: 
<https://www.kaggle.com/datasets/kingabzpro/gambling-behavior-bustabit/data>

This dataset contains behavior of gamblers using the online platform called 
Bustabit. This data was collected from 10/31/2016 to 12/10/2016. 

The rules of the game are that you bet money in Bits and you must cash
out before the game “busts.” Wins are calculated using the amount of the bet 
multiplied by the amount cashed out. For example, you bet 10 and you cash out at
3.5, so your win would be 35 minus what you put in, so 25. Bonuses are also added 
and must be multiplied by the bet. On Bustabit, the house also has a slight
advantage, where for every 1 out of 100 games, all players bust. 

## The dataset at a glance:

- 50000 rows; 43% NAs, meaning the player has busted before they were able to 
cash out. They chose a cash out value to play until, but busted before it was 
reached.
- 9 variables: ID, GameID, Username, Bet, CashedOut, Bonus, Profit, BustedAt, 
PlayDate

## Key variables of interest:

- **GameID:** an identifier randomized to match the exact time of play for each 
player login that has a set BustedAt multiplyer value
- **Username:** contains the unique nametag of each player
- **Bet:** the amount of bitcoin the player pays
- **CashedOut:** the multiplier value the player ends the game with that can be
used to find player’s Total Win
- **Bonus:** the percentage (as a decimal) for each game the player is rewarded with
- **Profit:** amount of bitcoin the player walks away with (calculated as 
(Bet x CashedOut) + (Bet + Bonus) - Bet); Profit of NA is 0, Profit of 0 is a 
loss or a "bust."
- **BustedAt:** where the multiplier has been randomly set to “bust”
- **PlayDate:** Year, month, day and time (Hours, minutes, seconds) of play

# Peep the Dataset
```{r message = FALSE}
library(tidyverse)
library(dplyr)
```

```{r read-data}
bitdf <- read.csv("/cloud/project/data/bustabit (1).csv")
```

```{r glimpse}
glimpse(bitdf)
```

## New Variables: 

- **Time** (separated from PlayDate variable)
- **Date** (separated from PlayDate variable)
- **FrequentPlayer** (binary variable I will calculate from Username)
- **NumberOfPlays** (calculated from Username)
- **Win** (binary variable where 1 = Profit greater than 0, 0 = Profit equal to 0)
- **ReturningPlayer** (calculated by Username where at player has played at LEAST
one time in the past prior to the current date of play)
- **AverageBet** (calculated using Username and Bet)
- **AverageCashOut** (calculated using Username and CashedOut)
- **PlayerNumber** (Player 1, Player 2, etc.; derived from GameID and Username)

# New Dataset:

```{r new-variables}
bitdfc <- bitdf %>%
  mutate(Profit = ifelse(is.na(Profit), 0, Profit))

bitdfc <- bitdfc %>%
  mutate(CashedOut = ifelse(is.na(CashedOut), 0, CashedOut))

bitdfc <- bitdfc %>%
  mutate(Bonus = ifelse(is.na(Bonus), 0, Bonus))

bitdfc <- bitdfc %>%
  group_by(Username) %>%
  mutate(NumberOfPlays = n()) %>%
  ungroup()

bitdfc <- bitdfc %>%
  mutate(ReturningPlayer = ifelse(NumberOfPlays > 1, 1, 0))

bitdfc <- bitdfc %>%
  group_by(Username) %>%
mutate(AverageBet = mean(Bet)) %>% 
  ungroup()

bitdfc <- bitdfc %>%
  group_by(Username) %>%
mutate(AverageCashOut = mean(CashedOut)) %>% 
  ungroup()

bitdfc <- bitdfc %>%
  mutate(Date = as.Date(PlayDate),
       Time = hms(substr(PlayDate, 12,19)))
bitdfc <- bitdfc %>%
  mutate(Time = hms::hms(Time))


bitdfc <- bitdfc %>%
  mutate(Win = ifelse(Profit > 0,1,0))

bitdfc <- bitdfc %>%
group_by(Username) %>%
mutate(FrequentPlayer = ifelse(n() >= 3, 1, 0)) %>%
ungroup()

glimpse(bitdfc)
```
This preprocessing involved:
- Coding NAs as 0 because these values were not invaluable. Within the data,
each NA represented an instance where a bet was made, but the player busted, and 
lost. Meaning, they made a profit, cash out, and bonus of 0.
- Creating the new variables to represent qualities of players that originally
were not uncovered
- Taking a 20% proportion of the data to work with because this data was originally
50,000 rows

# Aims

- **Aim 1:** Predict player profits from gambling. 

- **Aim 2:** Predict the "types of gamblers" players may be.

- **Aim 3:** Explore the win or loss outcome two players playing the same game.

# Exploratory Data Analysis

```{r library, message = FALSE}
library(dplyr)
library(ggplot2)
library(lubridate)
library(hutils)
library(tidymodels)
library(tidyverse)
library(doMC)
```

```{r explore-plots}
bitdf %>% ggplot(aes(x = Bet)) + geom_histogram(color = "black" , fill = "skyblue") + coord_cartesian(xlim = c(0, 50000)) +
  labs(title = "Amount Bet in Bits Across Bustabit Players",
      x = "Amounnt Bet (in Bits)", y = "Number of Players")

bitdf %>% ggplot(aes(x = Bet, y = Profit)) + geom_point(color = "darkgreen", fill = "black") + geom_smooth() +
  labs(title = "Relationship Between the Amount of Bits Bet and Player's Profit in Bustabit",
      x = "Bet (in Bits)", y = "Profit (in Bits)")

bitdf %>% ggplot(aes(x = CashedOut, y = Profit)) + geom_point(color = "maroon") + geom_smooth() + 
  labs(title = "Relationship Between the Cash Out Value and Player's Profit in Bustabit",
      x = "Value Cashed Out at (in Bits)", y = "Profit (in Bits)")
```
- Are wins and losses based on the amount bet?

Trends:
- majority of players bet smaller amounts
- the more players bet, the higher their profits are

## Are there Types of Players?

```{r}
bitdfc %>% ggplot(aes(x = ReturningPlayer, fill = as.factor(ReturningPlayer))) +
  geom_bar() +
  scale_fill_manual(values = c("red", "blue"), guide = "legend", 
                    labels = c("One Time Player", "Returning Player")) + 
  geom_text(stat = "count", aes(label = stat(count)), vjust = -0.5, size = 3) +
  labs(title = "Number of Returning Players to Bustabit",
       x = "Returning Player", y = "Number of Players", fill = "Player Type, Returning or Not") +
  theme_minimal()
```

```{r}
bitdfc %>% ggplot(aes(x = NumberOfPlays)) + geom_histogram(fill = "#FF1493") +
  labs(title = "Number of Times Players Gamble on BustaBit", x = "Times Played", y = "Count")
```
There are many different habits across players. It would be interesting to see
if there are groups of players with similar gambling habits.

```{r percentage-win-loses}
win_count <- sum(bitdfc$Win == 1)
loss_count <- sum(bitdfc$Win == 0)

counts <- c(Wins = win_count, Losses = loss_count)

pie(counts, labels = c("Wins", "Losses"), col = c("green", "red"))
```

Overall, we seem to have more Wins than Losses when looking at all games within
the dataset. However, will we be able to predict these Wins and Losses?

#  Aim 1: Predict player profits from gambling

## Predictors:
- Bet
- Bet, PlayDate, Bonus, Frequent player, Number of plays, Average bet,
Average Cash out

## Outcome Variable of interest:
- Profit (in bits)


# Modeling with Random Forest and Boosted Tree Models
- perform well on high-dimensional data
- we want to make a comparision between the performance of the two models
```{r build-a-model, message = FALSE}
library(tidyverse)
library(tidymodels)
library(hutils)
library(ranger)
library(xgboost)
library(doMC)
registerDoMC(cores = 4)
```

```{r}
#| cache: true

set.seed(1)
data_split <- initial_split(bitdfc)
train <- training(data_split)
test <- testing(data_split)

folds = vfold_cv(train, v=10)

simple_rec <- recipe(Profit ~  Bet, data = train) %>%
    step_dummy(all_nominal_predictors(), -all_outcomes()) %>%
    step_normalize(all_predictors(), -all_outcomes())

models <- list(random_forest =  rand_forest() %>% 
    set_engine("ranger") %>%
    set_mode("regression"),
               boosted_tree = boost_tree() %>% 
    set_engine("xgboost") %>%
    set_mode("regression"))

wf_set1 <- workflow_set(preproc = list(base = simple_rec), models = models)

fitted_models1 <- wf_set1 %>%
    workflow_map(resamples = folds, fn = "fit_resamples")

results_simple <- fitted_models1 %>%
    collect_metrics()

print(results_simple)


# Multiple Predictors of Profit
complex_rec <- recipe(Profit ~ FrequentPlayer + NumberOfPlays + ReturningPlayer + AverageBet + Bonus + Bet, data = train) %>%
    step_dummy(all_nominal_predictors(), -all_outcomes()) %>%
    step_normalize(all_predictors(), -all_outcomes())

models <- list(random_forest =  rand_forest() %>% 
    set_engine("ranger") %>%
    set_mode("regression"),
               boosted_tree = boost_tree() %>% 
    set_engine("xgboost") %>%
    set_mode("regression"))

wf_set2 <- workflow_set(preproc = list(base = complex_rec), models = models)

fitted_models2 <- wf_set2 %>%
    workflow_map(resamples = folds, fn = "fit_resamples")

results_complex <- fitted_models2 %>%
    collect_metrics()

print(results_complex)
```

```{r best-results, message = FALSE}
#| cache: true

library(lme4)

rf_model <- rand_forest(mtry = tune()) %>% set_engine("ranger") %>% set_mode("regression")

boosted_model <- boost_tree(trees = tune(), learn_rate = tune()) %>%
set_engine("xgboost") %>% set_mode("regression")
set.seed(123)

models_tuned <- list(
random_forest = rf_model, 
boosted_tree = boosted_model
)
wf_set_tuned <- workflow_set(preproc = list(fe = complex_rec), models = models_tuned,
cross = TRUE
)

library(finetune)

tune_results <- workflow_map(wf_set_tuned,
fn = "tune_race_anova",
resamples = folds,
grid = 3,
control = control_race(save_workflow = TRUE)
)

tune_metrics <- tune_results %>% collect_metrics() %>% select(-.estimator, -n)
best_results <- tune_metrics %>% group_by(model) %>% 
  filter(mean == min(mean)) %>% ungroup()

```

## Best Results from the Models
```{r}
print(best_results)
```

# Comparing the Models
```{r graph}
ggplot(best_results, aes(x = reorder(model, mean), y = mean, color = model)) +
  geom_point() +
  geom_errorbar(aes(ymin = mean - std_err, ymax = mean + std_err), width = 0.2) +
  coord_flip() +
  labs(title = "Best Model Performance Comparison", x = "Model", y = "Performance Estimate") +
  theme_minimal() +
  scale_color_brewer(palette = "Set1")
```
- Random Forest model had better overall performance, so we will use this model
on the test set

# Performance on the Test Data
```{r test-data}
#| cache: true

best_model <- tune_metrics %>% filter(.metric == "rmse") %>% arrange(mean) %>%
top_n(-1, mean)

best_workflow_id <- best_model$wflow_id

best_mod_results <- extract_workflow_set_result(tune_results,
id = "fe_random_forest")

best_params <- best_mod_results %>% select_best(metric = "rmse")

final_wf <- workflow() %>% add_recipe(complex_rec) %>% add_model(rf_model) %>% 
  finalize_workflow(best_params)

final_fit <- final_wf %>%
last_fit(split = initial_split(train))
fitted_wf <- final_fit$.workflow[[1]]

test_predictions <- predict(fitted_wf, new_data = test)
test_performance <- test_predictions %>% 
  bind_cols(test %>% select(Profit)) %>% 
  metrics(truth = Profit, estimate = .pred)

print(test_performance)
```

- This model has high error and is making predictions that are not precise
- High r-mean squared error, low r-squared value, and high mean absolute error
- It is difficult, if not impossible, to make accurate predictions of profit 
with any variables, which stands to prove the idea that gambling is purely 
probabilistic

# Aim 2: Predict the "types of players" individuals may be

## Predictors: 
- Bet, PlayDate, Bonus, Frequent player, Number of plays, Average bet,
Average Cash out

## Outcome Variables:
- Clusters that may illustrate different "Types of Players"

# Modeling with K-Means Clustering Analysis
- We want to visualize the trends within high-dimensional data, where we may
see patterns emerge that may represent types of players
```{r libraries}
library(tidyverse)
library(tidymodels)
library(hutils)
library(lubridate)
library(tidyclust)
```


```{r k-means}
#| cache: true

set.seed(1)
bitdfc_sample <- bitdfc %>% sample_frac(0.2)
metric_set_clus <- cluster_metric_set(sse_ratio, silhouette_avg)
kmeans_spec <- k_means(num_clusters = tune())
kmeans_rec <- recipe(~ FrequentPlayer + NumberOfPlays + ReturningPlayer + AverageBet + Bonus + Bet, data = bitdfc_sample) %>%
step_normalize(all_predictors())

kmeans_tune_wflow <- workflow() %>%
add_recipe(kmeans_rec) %>%
add_model(kmeans_spec)

grid <- tibble(num_clusters = 2:20)

folds <- vfold_cv(bitdfc_sample, v = 6)

tune_cluster <- tune_cluster(
kmeans_tune_wflow,
resamples = folds,
grid= grid,
metrics = metric_set_clus
)

kmeans_metrics <- tune_cluster %>% collect_metrics()
print(kmeans_metrics)
```


```{r metrics}
#| cache: true

kmeans_metrics %>% filter(.metric == "sse_ratio") %>%
ggplot(aes(x = num_clusters, y = mean)) +
geom_point() +
geom_line() + labs(title = "Elbow Plot")
```
- About 5 clusters seem optimal based on the curve of the plot 

```{r}
kmeans_metrics %>%
filter(.metric == "silhouette_avg") %>%
ggplot(aes(x = num_clusters, y = mean)) +
geom_point () +
geom_line () +
theme_minimal() + labs(title = "Silhouette Average Plot")
```
- About 5 clusters seem optimal, as this is where we begin to see a steep fall

```{r}
kmeans_spec <- k_means(num_clusters = 5)
optimal_kmeansfit <- workflow() %>%
add_recipe(kmeans_rec) %>%
add_model(kmeans_spec) %>%
fit(data = bitdfc_sample)
```


# Visualizing High-Dimensional Data

```{r spot-the-trends}
kmeans_clusters <- optimal_kmeansfit %>%
predict(new_data = bitdfc_sample) %>%
bind_cols(bitdfc_sample) %>%
pull(.pred_cluster)

bitdfc_k <- bitdfc_sample %>% mutate(.cluster_k = kmeans_clusters) 

ggplot(bitdfc_k, aes(x = AverageBet, y = Profit, color = factor(.cluster_k))) +
geom_point() + labs(x = "Number of Plays", y = "Average Bet", color = "Cluster") +
theme_minimal() + labs(title = "Number of Plays as a Predictor of Average Bet,
                       by Player Type")
```

Here, we cannot see the trends between players very well. So, we are going to 
use dimension reduction techniques to capture the first two Principle
components of the data. 

# Pulling Principle Components
```{r PCA}
pca_rec <- recipe(~ FrequentPlayer + NumberOfPlays + ReturningPlayer + AverageBet + Bonus + Bet, data = bitdfc_sample) %>%
step_normalize(all_predictors())%>%
step_pca(all_numeric_predictors(), keep_original_cols = TRUE)
pca_prep <- prep(pca_rec)
pca_data <- bake(pca_prep, bitdfc_sample)

pca_data <-cbind(pca_data,
optimal_kmeansfit %>% extract_cluster_assignment() %>%
rename(.km_cluster = .cluster))

pcacentroids1 <- pca_data %>%
group_by(.km_cluster) %>%
summarise(across(c(PC1, PC2, PC3), mean))

pca_data %>%
  ggplot(aes(x = PC1, y = PC2, color = .km_cluster)) +
  geom_point() +
  geom_point(data = pcacentroids1, size = 5, shape = 'X') +
  theme_minimal()
```


# Aim 3: Explore the win or loss outcome two players playing the same game

## Predictor Variables:
- Bet, PlayDate, Bonus, Frequent player, Number of plays, Average bet,
Average Cash out

## Outcome Variable:
- Win (binary variable, where 0 = loss, 1 = win)


# Modeling with K-Nearest-Neighbors (kNN)
- Used to make predictions of a binary outcome (Win)
```{r creating-new-dataset}
bitdfc2 <- bitdfc %>%
  group_by(GameID) %>%
  mutate(PlayerNumber = paste0("Player ", row_number())) %>%
  ungroup()

gameid_counts <- bitdfc2 %>%
  group_by(GameID) %>%
  summarise(num_players = n_distinct(PlayerNumber))

selected_gameids <- gameid_counts %>%
  filter(num_players >= 2) %>%
  pull(GameID)

bitdfc2 <- bitdfc2 %>%
  filter(GameID %in% selected_gameids)
```

```{r KNN, message = FALSE}
#| cache: true

library(kknn)
library(discrim)
bitdfc2$Win <- as.factor(bitdfc2$Win)
rec <- recipe(Win ~ Bet + Bonus + AverageBet + AverageCashOut + NumberOfPlays + 
                FrequentPlayer, data = bitdfc2) %>% 
step_normalize(all_predictors())

set.seed(1)

multi_metric <- metric_set(kap, accuracy)

data_split2 <- initial_split(bitdfc2, strata = PlayerNumber)
train2 <- training(data_split2)
test2 <- testing(data_split2)

folds2 <- vfold_cv(
  data = train2,
  v = 5,
  strata = Win,
  repeats = 1
  )

knn_spec <- nearest_neighbor(
    mode = "classification",
    engine = "kknn",
    neighbors = tune())
  
knn_wf <- workflow() %>% 
  add_recipe(rec) %>% 
  add_model(knn_spec) 

k_grid <- tibble(neighbors=seq(3,20,1))

knn_fit_rs <- knn_wf %>% 
  tune_grid(resamples = folds2,
            grid = k_grid,
                metrics = multi_metric)

collect_metrics(knn_fit_rs)

chosen_n <- knn_fit_rs %>% 
  select_best(metric = "accuracy")

final_knn <- finalize_workflow(knn_wf, chosen_n)

knn_fit <- fit(final_knn, train2)

knn_fit %>% 
  augment(test2) %>% 
  metrics(Win, .pred_class)
```
- The model was able to predict Win around 81.9% of the time, with these 
constraints.

```{r percentage-of-correctness}
predictions <- predict(knn_fit, new_data = test2)

prediction_data <- data.frame(Predicted = predictions$.pred_class, Actual = test2$Win)

correctness <- ifelse(prediction_data$Predicted == prediction_data$Actual, 
                      "Correct", "Incorrect")

correct_counts <- sum(correctness == "Correct")
incorrect_counts <- sum(correctness == "Incorrect")

total <- correct_counts + incorrect_counts
correct_percentage <- round((correct_counts / total) * 100, 2)
incorrect_percentage <- round((incorrect_counts / total) * 100, 2)

percentages_df <- data.frame(Category = c("Correct", "Incorrect"), 
                             Percentage = c(correct_percentage, incorrect_percentage))

pie(percentages_df$Percentage, labels = paste(percentages_df$Category, 
                                              percentages_df$Percentage, "%"), 
    col = c("green", "red"), main = "Predicted vs. Actual Values")
```

We will now visualize if these predictions differ by Player number,
as we stratified by this value.

```{r}
prediction_data2 <- data.frame(Predicted = predictions$.pred_class, Actual = test2$Win,
                              PlayerNumber = test2$PlayerNumber, GameID = test2$GameID)

prediction_data2 <- prediction_data2 %>% mutate(correctness <- ifelse(prediction_data$Predicted == prediction_data$Actual, 
                      1, 0))

prediction_data2 %>% ggplot(aes(x = PlayerNumber, fill = correctness)) + geom_bar()
```

- Illustrates the predicted correctness of wins for each Player Number
- It does not seem that there are any predicted differences between players 1
and 2 when they have the same set initial conditions determined by Game ID.

# Conclusions

- These models have the potential to be applicable to other gambling domains, 
besides predicting exact profits. 
-With more information on players, trends between gamblers may become more
apparent.
- It may be possible that certain qualities of gamblers can be used to predict 
their wins and losses, which would have major implications in real-life
- For example, knowing one's strengths and weaknesses as a more "frequent 
ßplayer,"
may alter how they gamble in future occurrences. 